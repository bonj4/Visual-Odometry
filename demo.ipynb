{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libs included\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import plotly.express as px\n",
    "print(\"libs included\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset():\n",
    "    def __init__(self,seq=\"00\"):\n",
    "        \n",
    "        self.sequences_dir = 'sequences/{}/'.format(seq)\n",
    "        self.poses_dir = 'poses/{}.txt'.format(seq)\n",
    "        \n",
    "        self.left_img=(cv2.imread(self.sequences_dir+\"image_0/\"+i,0) for i in sorted(os.listdir(self.sequences_dir+\"image_0\")) if i.endswith(\"png\"))\n",
    "        self.right_img=(cv2.imread(self.sequences_dir+\"image_1/\"+i,0) for i in sorted(os.listdir(self.sequences_dir+\"image_1\")) if i.endswith(\"png\"))\n",
    "        self.poses=pd.read_csv(self.poses_dir, delimiter=' ', header=None)\n",
    "        self.calib=pd.read_csv(self.sequences_dir+\"calib.txt\", delimiter=' ', header=None, index_col=0)\n",
    "        \n",
    "        self.p0=np.array(self.calib.iloc[0]).reshape((3,4))\n",
    "        self.p1=np.array(self.calib.iloc[1]).reshape((3,4))\n",
    "        self.Tr = np.array(self.calib.iloc[-1]).reshape((3,4))\n",
    "\n",
    "\n",
    "    def next_imgs(self):\n",
    "        return next(self.left_img),next(self.right_img)\n",
    "    def reset_images(self):\n",
    "        self.left_img=(cv2.imread(self.sequences_dir+\"image_0/\"+i,0) for i in sorted(os.listdir(self.sequences_dir+\"image_0\")) if i.endswith(\"png\"))\n",
    "        self.right_img=(cv2.imread(self.sequences_dir+\"image_1/\"+i,0) for i in sorted(os.listdir(self.sequences_dir+\"image_1\")) if i.endswith(\"png\"))\n",
    "    \n",
    "data=dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Visualization left camera's frames\n",
    "for i in range(len(os.listdir(data.sequences_dir+\"image_0\"))):\n",
    "    cv2.imshow(\"winname\", data.next_imgs()[0])\n",
    "    key=cv2.waitKey(27)\n",
    "    if key==ord('q'):\n",
    "        data.reset_images()\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_left_disparity_map(img_left, img_right, matcher='bm', verbose=False):\n",
    "    \n",
    "    sad_window = 6\n",
    "    num_disparities = sad_window * 16\n",
    "    block_size = 11\n",
    "    matcher_name = matcher\n",
    "    \n",
    "    if matcher_name == 'bm':\n",
    "        matcher = cv2.StereoBM_create(numDisparities=num_disparities,\n",
    "                                      blockSize=block_size)\n",
    "        \n",
    "    elif matcher_name == 'sgbm':\n",
    "        matcher = cv2.StereoSGBM_create(numDisparities=num_disparities,\n",
    "                                        minDisparity=0,\n",
    "                                        blockSize=block_size,\n",
    "                                        P1 = 8 * 1 * block_size ** 2,\n",
    "                                        P2 = 32 * 1 * block_size ** 2,\n",
    "                                        mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY)\n",
    "    \n",
    "\n",
    "        \n",
    "    start = time.perf_counter()\n",
    "    disp_left = matcher.compute(img_left, img_right).astype(np.float32)/16\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Time to compute disparity map using Stereo{matcher_name.upper()}', end-start)\n",
    "        \n",
    "    return disp_left\n",
    "\n",
    "# disp = compute_left_disparity_map(data.next_imgs()[0],\n",
    "#                                   data.next_imgs()[1],\n",
    "#                                   matcher='sgbm',\n",
    "#                                   verbose=True)\n",
    "# plt.figure(figsize=(11,7))\n",
    "# plt.imshow(disp);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Visualization disparity frames\n",
    "for i in range(len(os.listdir(data.sequences_dir+\"image_0\"))):\n",
    "    disp = compute_left_disparity_map(data.next_imgs()[0],\n",
    "                                      data.next_imgs()[1],\n",
    "                                      matcher='bm',\n",
    "                                      verbose=False)\n",
    "    disp/=disp.max()\n",
    "    disp*=255.\n",
    "    cv2.imshow(\"winname\", disp.astype(np.uint8))\n",
    "    key=cv2.waitKey(27)\n",
    "    if key==ord('q'):\n",
    "        data.reset_images()\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_projection_matrix(p):\n",
    "    k, r, t, _, _, _, _ = cv2.decomposeProjectionMatrix(p)\n",
    "    t = (t / t[3])[:3]\n",
    "    return k, r, t\n",
    "\n",
    "def calc_depth_map(disp_left, k_left, t_left, t_right, rectified=True):\n",
    "    \n",
    "    if rectified:\n",
    "        b = t_right[0] - t_left[0]\n",
    "    else:\n",
    "        b = t_left[0] - t_right[0]\n",
    "        \n",
    "    f = k_left[0][0]\n",
    "    \n",
    "    disp_left[disp_left == 0.0] = 0.1\n",
    "    disp_left[disp_left == -1.0] = 0.1\n",
    "    \n",
    "    depth_map = np.ones(disp_left.shape)\n",
    "    depth_map = f * b / disp_left\n",
    "    \n",
    "    return depth_map\n",
    "\n",
    "# k_left, r_left, t_left = decompose_projectionwert_wertmatrix(data.p0)\n",
    "# k_right, r_right, t_right = decompose_projection_matrix(data.p1)\n",
    "\n",
    "# depth = calc_depth_map(disp, k_left, t_left, t_right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(os.listdir(data.sequences_dir+\"image_0\"))):\n",
    "    k_left, r_left, t_left = decompose_projection_matrix(data.p0)\n",
    "    k_right, r_right, t_right = decompose_projection_matrix(data.p1)\n",
    "    disp = compute_left_disparity_map(data.next_imgs()[0],\n",
    "                                      data.next_imgs()[1],\n",
    "                                      matcher='sgbm',\n",
    "                                      verbose=True)\n",
    "    depth = calc_depth_map(disp, k_left, t_left, t_right)\n",
    "    depth/=depth.max()\n",
    "    depth*=255\n",
    "    cv2.imshow(\"winname\", disp.astype(np.uint8))\n",
    "    key=cv2.waitKey(27)\n",
    "    if key==ord('q'):\n",
    "        data.reset_images()\n",
    "        break\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image, detector='sift',GoodP=False,mask=None):\n",
    "        \n",
    "    if detector == 'sift':\n",
    "        det = cv2.SIFT_create()\n",
    "    elif detector == 'surf':\n",
    "        det = cv2.xfeatures2d.SURF_create()\n",
    "    elif detector == 'orb':\n",
    "        det = cv2.ORB_create()\n",
    "    if GoodP:\n",
    "        pts = cv2.goodFeaturesToTrack(image, 3000, qualityLevel=0.01, minDistance=7)\n",
    "        kps = [cv2.KeyPoint(x=f[0][0], y=f[0][1], size=15) for f in pts]\n",
    "        kp, des = det.compute(image, kps)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        kp, des = det.detectAndCompute(image, mask)\n",
    "    kp=np.array([(k.pt[0], k.pt[1]) for k in kp])\n",
    "    return kp, des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features(des1, des2, matching='BF', detector='sift', k=2):\n",
    "    \n",
    "    if matching == 'BF':\n",
    "        if detector == 'sift' or detector == 'surf':\n",
    "            matcher = cv2.BFMatcher_create(cv2.NORM_L2, crossCheck=False)\n",
    "        elif detector == 'orb':\n",
    "            matcher = cv2.BFMatcher_create(cv2.NORM_HAMMING2, crossCheck=False)\n",
    "        matches = matcher.knnMatch(des1, des2, k=k)\n",
    "    elif matching == 'FLANN':\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "        matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    \n",
    "\n",
    "        \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_matches_distance(matches, dist_threshold=0.5):\n",
    "    filtered_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance <= dist_threshold * n.distance:\n",
    "            filtered_matches.append(m)\n",
    "            \n",
    "    return filtered_matches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawMatches(img1, img2,kp1,kp2 ,matches):\n",
    "    merge_img=cv2.hconcat([img1,img2])\n",
    "    for m in matches:\n",
    "        p1 = kp1[m.queryIdx]\n",
    "        p2 = kp2[m.trainIdx]\n",
    "\n",
    "        x1,y1=map(lambda x:int(round(x)),p1)\n",
    "        x2,y2=map(lambda x:int(round(x)),p2)\n",
    "        cv2.circle(merge_img, (x1,y1), 3, (255))\n",
    "\n",
    "        cv2.circle(merge_img, (img1.shape[1]+x2,y2), 3,(255))\n",
    "        cv2.line(merge_img, (x1,y1), (img1.shape[1]+x2,y2), (255))\n",
    "    return merge_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6178 fps\n",
      "1.2471 fps\n",
      "1.691 fps\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(os.listdir(data.sequences_dir+\"image_0\"))):\n",
    "    s=time.perf_counter()\n",
    "    img_l=data.next_imgs()[0]\n",
    "    if not i:\n",
    "        last_img_l=img_l\n",
    "        continue\n",
    "    keypoints_1, descriptors_1 = extract_features(img_l,detector='sift',mask=None)\n",
    "    keypoints_2, descriptors_2 = extract_features(last_img_l,mask=None)\n",
    "    matches=match_features(descriptors_1, descriptors_2, matching='BF', detector='sift', k=2)\n",
    "    matches=filter_matches_distance(matches,dist_threshold=0.5)\n",
    "    image_matches = drawMatches(img_l, last_img_l, keypoints_1,keypoints_2,matches)\n",
    "    cv2.namedWindow(\"winname\",cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(\"winname\", image_matches)\n",
    "    key=cv2.waitKey(27)\n",
    "    if key==ord('q'):\n",
    "        data.reset_images()\n",
    "        break\n",
    "    last_img_l=img_l\n",
    "    print(round(1/(time.perf_counter()-s),4), \"fps\")\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_matches(image1, kp1, image2, kp2, match):\n",
    "    image_matches = cv2.drawMatches(image1, kp1, image2, kp2, match, None, flags=2)\n",
    "    plt.figure(figsize=(16, 6), dpi=100)\n",
    "    plt.imshow(image_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_motion(matches, kp1, kp2, k, depth1=None, max_depth=3000):\n",
    "    \n",
    "    rmat = np.eye(3)\n",
    "    tvec = np.zeros((3, 1))\n",
    "    \n",
    "    image1_points = np.float32([kp1[m.queryIdx] for m in matches])\n",
    "    image2_points = np.float32([kp2[m.trainIdx] for m in matches])\n",
    "    if depth1 is not None :\n",
    "        cx = k[0, 2]\n",
    "        cy = k[1, 2]\n",
    "        fx = k[0, 0]\n",
    "        fy = k[1, 1]\n",
    "        \n",
    "        object_points = np.zeros((0, 3))\n",
    "        delete = []\n",
    "        \n",
    "        for i, (u, v) in enumerate(image1_points):\n",
    "            z = depth1[int(round(v)), int(round(u))]\n",
    "            \n",
    "            if z > max_depth:\n",
    "                delete.append(i)\n",
    "                continue\n",
    "                \n",
    "            x = z * (u - cx) / fx\n",
    "            y = z * (v - cy) / fy\n",
    "            object_points = np.vstack([object_points, np.array([x, y, z])])            \n",
    "        image1_points = np.delete(image1_points, delete, 0)\n",
    "        image2_points = np.delete(image2_points, delete, 0)\n",
    "        \n",
    "        _, rvec, tvec, inliers = cv2.solvePnPRansac(object_points, image2_points, k, None)\n",
    "        rmat = cv2.Rodrigues(rvec)[0]\n",
    "    else:\n",
    "        # Compute the essential matrix\n",
    "        essential_matrix, mask = cv2.findEssentialMat(image1_points,image2_points , focal=1.0, pp=(0., 0.), method=cv2.RANSAC, prob=0.999, threshold=3.0)\n",
    "        # Recover the relative pose of the cameras\n",
    "        _, rmat, tvec, mask = cv2.recoverPose(essential_matrix, image1_points, image2_points)\n",
    "    return rmat, tvec, image1_points, image2_points\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(m1,m2):\n",
    "    m1,m2=m1.flatten(),m2.flatten()\n",
    "    return (sum((m2-m1)** 2))/m1.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16787234999992506 s\n"
     ]
    }
   ],
   "source": [
    "s=time.perf_counter()\n",
    "\n",
    "last_img_l,last_img_r=data.next_imgs()\n",
    "\n",
    "# disp=compute_left_disparity_map(last_img_l,last_img_r)\n",
    "disp=compute_left_disparity_map(last_img_l,\n",
    "                                  last_img_r,\n",
    "                                  matcher='sgbm')\n",
    "\n",
    "k_left, r_left, t_left = decompose_projection_matrix(data.p0)\n",
    "k_right, r_right, t_right = decompose_projection_matrix(data.p1)\n",
    "\n",
    "depth = calc_depth_map(disp, k_left, t_left, t_right)\n",
    "\n",
    "\n",
    "img_l=data.next_imgs()[0]\n",
    "keypoints_1, descriptors_1 = extract_features(img_l,detector='orb',GoodP=True,mask=None,)\n",
    "keypoints_2, descriptors_2 = extract_features(last_img_l,detector='orb',GoodP=True,mask=None)\n",
    "matches=match_features(descriptors_1, descriptors_2, matching='BF', detector='orb',)\n",
    "matches=filter_matches_distance(matches,0.3)\n",
    "image_matches = drawMatches(img_l, last_img_l, keypoints_1,keypoints_2,matches)\n",
    "# plt.figure(figsize=(16, 6), dpi=100)\n",
    "# plt.imshow(image_matches)\n",
    "k, r, t, _, _, _, _ = cv2.decomposeProjectionMatrix(data.p0)\n",
    "rmat, tvec, image1_points, image2_points =estimate_motion(matches=matches,kp1=keypoints_1,kp2=keypoints_2,k=k,depth1=depth)\n",
    "transformation_matrix = np.hstack([rmat, tvec])\n",
    "print((time.perf_counter()-s),'s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=np.array(data.poses.iloc[1]).reshape((3,4)).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002988347569563586\n"
     ]
    }
   ],
   "source": [
    "print(mse(m1,transformation_matrix))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sgbm,sift ,gp=false  \n",
    "süre:1.55\n",
    "mse: 0.0026911559834271546\n",
    "\n",
    "sgbm,sift ,gp=True  \n",
    "mse: 0.0036083649504361514\n",
    "\n",
    "sgbm,orb ,gp=True  \n",
    "süre:0.29\n",
    "mse: 0.002988347569563586\n",
    "\n",
    "sgbm,orb ,gp=False  \n",
    "süre:0.31\n",
    "mse: 0.0036566450577993673\n",
    "\n",
    "bm,orb ,gp=True  \n",
    "mse: 0.0035251248359337507\n",
    "\n",
    "bm, orb ,gp=False  \n",
    "mse: 0.0032920821980853827\n",
    "\n",
    "bm,sift ,gp=false  \n",
    "mse: 0.0028242799111296573\n",
    "\n",
    "bm,sift ,gp=True  \n",
    "mse: 0.005100526103442996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VisualOdometry(matcher='sgbm',detector='orb',GoodP=True,dist_threshol=None):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea6ce58c2ccc6c680274b9bd63eeec8dbef13a167de91e72bd94ccc7b6917842"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
